{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\azeem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\azeem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from colorama import Fore, init\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>732</td>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#ScienceFairWinner #HighSchoolScience</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>733</td>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#SurpriseCelebration #HighSchoolFriendship</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>734</td>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#CommunityGiving #HighSchoolPhilanthropy</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>735</td>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CulturalCelebration #HighSchoolUnity</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>732</td>\n",
       "      <td>736</td>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#VirtualEntertainment #HighSchoolPositivity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0               0           0   \n",
       "1               1           1   \n",
       "2               2           2   \n",
       "3               3           3   \n",
       "4               4           4   \n",
       "..            ...         ...   \n",
       "727           728         732   \n",
       "728           729         733   \n",
       "729           730         734   \n",
       "730           731         735   \n",
       "731           732         736   \n",
       "\n",
       "                                                  Text    Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1     Traffic was terrible this morning.           ...   Negative     \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3     Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "..                                                 ...          ...   \n",
       "727  Collaborating on a science project that receiv...       Happy    \n",
       "728  Attending a surprise birthday party organized ...       Happy    \n",
       "729  Successfully fundraising for a school charity ...       Happy    \n",
       "730  Participating in a multicultural festival, cel...       Happy    \n",
       "731  Organizing a virtual talent show during challe...       Happy    \n",
       "\n",
       "               Timestamp                                   User     Platform  \\\n",
       "0    2023-01-15 12:30:00                          User123          Twitter     \n",
       "1    2023-01-15 08:45:00                          CommuterX        Twitter     \n",
       "2    2023-01-15 15:45:00                          FitnessFan      Instagram    \n",
       "3    2023-01-15 18:20:00                          AdventureX       Facebook    \n",
       "4    2023-01-15 19:55:00                          ChefCook        Instagram    \n",
       "..                   ...                                    ...          ...   \n",
       "727  2017-08-18 18:20:00       ScienceProjectSuccessHighSchool     Facebook    \n",
       "728  2018-06-22 14:15:00            BirthdayPartyJoyHighSchool    Instagram    \n",
       "729  2019-04-05 17:30:00   CharityFundraisingTriumphHighSchool      Twitter    \n",
       "730  2020-02-29 20:45:00    MulticulturalFestivalJoyHighSchool     Facebook    \n",
       "731  2020-11-15 15:15:00    VirtualTalentShowSuccessHighSchool    Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  \\\n",
       "0        #Nature #Park                                  15.0   30.0   \n",
       "1        #Traffic #Morning                               5.0   10.0   \n",
       "2        #Fitness #Workout                              20.0   40.0   \n",
       "3        #Travel #Adventure                              8.0   15.0   \n",
       "4        #Cooking #Food                                 12.0   25.0   \n",
       "..                                             ...       ...    ...   \n",
       "727         #ScienceFairWinner #HighSchoolScience       20.0   39.0   \n",
       "728    #SurpriseCelebration #HighSchoolFriendship       25.0   48.0   \n",
       "729      #CommunityGiving #HighSchoolPhilanthropy       22.0   42.0   \n",
       "730         #CulturalCelebration #HighSchoolUnity       21.0   43.0   \n",
       "731   #VirtualEntertainment #HighSchoolPositivity       24.0   47.0   \n",
       "\n",
       "          Country  Year  Month  Day  Hour  \n",
       "0       USA        2023      1   15    12  \n",
       "1       Canada     2023      1   15     8  \n",
       "2     USA          2023      1   15    15  \n",
       "3       UK         2023      1   15    18  \n",
       "4      Australia   2023      1   15    19  \n",
       "..            ...   ...    ...  ...   ...  \n",
       "727            UK  2017      8   18    18  \n",
       "728           USA  2018      6   22    14  \n",
       "729        Canada  2019      4    5    17  \n",
       "730            UK  2020      2   29    20  \n",
       "731           USA  2020     11   15    15  \n",
       "\n",
       "[732 rows x 15 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\Masters\\Semester 4\\Machine Learning\\sentiment-analysis\\dataset\\sentimentdataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Pre-processing steps (checking data-types of columns, null values, null percentages etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_69213_row0_col2, #T_69213_row0_col3, #T_69213_row1_col2, #T_69213_row1_col3, #T_69213_row2_col2, #T_69213_row2_col3, #T_69213_row3_col2, #T_69213_row3_col3, #T_69213_row4_col2, #T_69213_row4_col3, #T_69213_row5_col2, #T_69213_row5_col3, #T_69213_row6_col2, #T_69213_row6_col3, #T_69213_row7_col2, #T_69213_row7_col3, #T_69213_row8_col2, #T_69213_row8_col3, #T_69213_row9_col2, #T_69213_row9_col3, #T_69213_row10_col2, #T_69213_row10_col3, #T_69213_row11_col2, #T_69213_row11_col3, #T_69213_row12_col2, #T_69213_row12_col3, #T_69213_row13_col2, #T_69213_row13_col3, #T_69213_row14_col2, #T_69213_row14_col3 {\n",
       "  background-color: #8dd3c7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_69213\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_69213_level0_col0\" class=\"col_heading level0 col0\" >features</th>\n",
       "      <th id=\"T_69213_level0_col1\" class=\"col_heading level0 col1\" >dtypes</th>\n",
       "      <th id=\"T_69213_level0_col2\" class=\"col_heading level0 col2\" >NaN count</th>\n",
       "      <th id=\"T_69213_level0_col3\" class=\"col_heading level0 col3\" >NaN percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_69213_row0_col0\" class=\"data row0 col0\" >Unnamed: 0.1</td>\n",
       "      <td id=\"T_69213_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_69213_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_69213_row1_col0\" class=\"data row1 col0\" >Unnamed: 0</td>\n",
       "      <td id=\"T_69213_row1_col1\" class=\"data row1 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_69213_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_69213_row2_col0\" class=\"data row2 col0\" >Text</td>\n",
       "      <td id=\"T_69213_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_69213_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_69213_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_69213_row3_col0\" class=\"data row3 col0\" >Sentiment</td>\n",
       "      <td id=\"T_69213_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_69213_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_69213_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_69213_row4_col0\" class=\"data row4 col0\" >Timestamp</td>\n",
       "      <td id=\"T_69213_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_69213_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_69213_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_69213_row5_col0\" class=\"data row5 col0\" >User</td>\n",
       "      <td id=\"T_69213_row5_col1\" class=\"data row5 col1\" >object</td>\n",
       "      <td id=\"T_69213_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_69213_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_69213_row6_col0\" class=\"data row6 col0\" >Platform</td>\n",
       "      <td id=\"T_69213_row6_col1\" class=\"data row6 col1\" >object</td>\n",
       "      <td id=\"T_69213_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_69213_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_69213_row7_col0\" class=\"data row7 col0\" >Hashtags</td>\n",
       "      <td id=\"T_69213_row7_col1\" class=\"data row7 col1\" >object</td>\n",
       "      <td id=\"T_69213_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_69213_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_69213_row8_col0\" class=\"data row8 col0\" >Retweets</td>\n",
       "      <td id=\"T_69213_row8_col1\" class=\"data row8 col1\" >float64</td>\n",
       "      <td id=\"T_69213_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_69213_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_69213_row9_col0\" class=\"data row9 col0\" >Likes</td>\n",
       "      <td id=\"T_69213_row9_col1\" class=\"data row9 col1\" >float64</td>\n",
       "      <td id=\"T_69213_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_69213_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_69213_row10_col0\" class=\"data row10 col0\" >Country</td>\n",
       "      <td id=\"T_69213_row10_col1\" class=\"data row10 col1\" >object</td>\n",
       "      <td id=\"T_69213_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_69213_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_69213_row11_col0\" class=\"data row11 col0\" >Year</td>\n",
       "      <td id=\"T_69213_row11_col1\" class=\"data row11 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "      <td id=\"T_69213_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_69213_row12_col0\" class=\"data row12 col0\" >Month</td>\n",
       "      <td id=\"T_69213_row12_col1\" class=\"data row12 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row12_col2\" class=\"data row12 col2\" >0</td>\n",
       "      <td id=\"T_69213_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_69213_row13_col0\" class=\"data row13 col0\" >Day</td>\n",
       "      <td id=\"T_69213_row13_col1\" class=\"data row13 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row13_col2\" class=\"data row13 col2\" >0</td>\n",
       "      <td id=\"T_69213_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69213_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_69213_row14_col0\" class=\"data row14 col0\" >Hour</td>\n",
       "      <td id=\"T_69213_row14_col1\" class=\"data row14 col1\" >int64</td>\n",
       "      <td id=\"T_69213_row14_col2\" class=\"data row14 col2\" >0</td>\n",
       "      <td id=\"T_69213_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x208a946be50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def null_count():\n",
    "    return pd.DataFrame({'features': df.columns,\n",
    "                'dtypes': df.dtypes.values,\n",
    "                'NaN count': df.isnull().sum().values,\n",
    "                'NaN percentage': df.isnull().sum().values/df.shape[0]}).style.background_gradient(cmap='Set3',low=0.1,high=0.01)\n",
    "null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Text', 'Sentiment', 'Timestamp', 'User',\n",
       "       'Platform', 'Hashtags', 'Retweets', 'Likes', 'Country', 'Year', 'Month',\n",
       "       'Day', 'Hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0.1: 732 distinct values\n",
      "Unnamed: 0: 732 distinct values\n",
      "Text: 707 distinct values\n",
      "Sentiment: 279 distinct values\n",
      "Timestamp: 683 distinct values\n",
      "User: 685 distinct values\n",
      "Platform: 4 distinct values\n",
      "Hashtags: 697 distinct values\n",
      "Retweets: 26 distinct values\n",
      "Likes: 38 distinct values\n",
      "Country: 115 distinct values\n",
      "Year: 14 distinct values\n",
      "Month: 12 distinct values\n",
      "Day: 31 distinct values\n",
      "Hour: 22 distinct values\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    num_distinct_values = len(df[column].unique())\n",
    "    print(f\"{column}: {num_distinct_values} distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Hashtags','Day', 'Hour','Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text            Timestamp  \\\n",
       "0     Enjoying a beautiful day at the park!        ...  2023-01-15 12:30:00   \n",
       "1     Traffic was terrible this morning.           ...  2023-01-15 08:45:00   \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...  2023-01-15 15:45:00   \n",
       "3     Excited about the upcoming weekend getaway!  ...  2023-01-15 18:20:00   \n",
       "4     Trying out a new recipe for dinner tonight.  ...  2023-01-15 19:55:00   \n",
       "..                                                 ...                  ...   \n",
       "727  Collaborating on a science project that receiv...  2017-08-18 18:20:00   \n",
       "728  Attending a surprise birthday party organized ...  2018-06-22 14:15:00   \n",
       "729  Successfully fundraising for a school charity ...  2019-04-05 17:30:00   \n",
       "730  Participating in a multicultural festival, cel...  2020-02-29 20:45:00   \n",
       "731  Organizing a virtual talent show during challe...  2020-11-15 15:15:00   \n",
       "\n",
       "                                      User     Platform  Retweets  Likes  \\\n",
       "0                            User123          Twitter        15.0   30.0   \n",
       "1                            CommuterX        Twitter         5.0   10.0   \n",
       "2                            FitnessFan      Instagram       20.0   40.0   \n",
       "3                            AdventureX       Facebook        8.0   15.0   \n",
       "4                            ChefCook        Instagram       12.0   25.0   \n",
       "..                                     ...          ...       ...    ...   \n",
       "727       ScienceProjectSuccessHighSchool     Facebook       20.0   39.0   \n",
       "728            BirthdayPartyJoyHighSchool    Instagram       25.0   48.0   \n",
       "729   CharityFundraisingTriumphHighSchool      Twitter       22.0   42.0   \n",
       "730    MulticulturalFestivalJoyHighSchool     Facebook       21.0   43.0   \n",
       "731    VirtualTalentShowSuccessHighSchool    Instagram       24.0   47.0   \n",
       "\n",
       "          Country  Year  Month  \n",
       "0       USA        2023      1  \n",
       "1       Canada     2023      1  \n",
       "2     USA          2023      1  \n",
       "3       UK         2023      1  \n",
       "4      Australia   2023      1  \n",
       "..            ...   ...    ...  \n",
       "727            UK  2017      8  \n",
       "728           USA  2018      6  \n",
       "729        Canada  2019      4  \n",
       "730            UK  2020      2  \n",
       "731           USA  2020     11  \n",
       "\n",
       "[732 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform\n",
       "Instagram     258\n",
       "Facebook      231\n",
       "Twitter       128\n",
       "Twitter       115\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Platform'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Platform'] = df['Platform'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "USA               59\n",
       "USA               55\n",
       "UK                49\n",
       "Canada            44\n",
       "Australia         41\n",
       "                  ..\n",
       "Netherlands        1\n",
       "USA                1\n",
       "Germany            1\n",
       "France             1\n",
       "USA                1\n",
       "Name: count, Length: 115, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = df['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "df['Day_of_Week'] = df['Timestamp'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            USA\n",
       "1         Canada\n",
       "2            USA\n",
       "3             UK\n",
       "4      Australia\n",
       "         ...    \n",
       "727           UK\n",
       "728          USA\n",
       "729       Canada\n",
       "730           UK\n",
       "731          USA\n",
       "Name: Country, Length: 732, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    1: 'Januari',\n",
    "    2: 'Februari',\n",
    "    3: 'Maret',\n",
    "    4: 'April',\n",
    "    5: 'Mei',\n",
    "    6: 'Juni',\n",
    "    7: 'Juli',\n",
    "    8: 'Agustus',\n",
    "    9: 'September',\n",
    "    10: 'Oktober',\n",
    "    11: 'November',\n",
    "    12: 'Desember'\n",
    "}\n",
    "\n",
    "df['Month'] = df['Month'].map(month_mapping)\n",
    "\n",
    "df['Month'] = df['Month'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  \n",
    "    text = \" \".join(text.split())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "   \n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>Januari</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>enjoy beauti day park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>Januari</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>traffic terribl morn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>Januari</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>finish amaz workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>Januari</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>excit upcom weekend getaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>Januari</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>tri new recip dinner tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agustus</td>\n",
       "      <td>Friday</td>\n",
       "      <td>collabor scienc project receiv recognit region...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>Juni</td>\n",
       "      <td>Friday</td>\n",
       "      <td>attend surpris birthday parti organ friend sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>April</td>\n",
       "      <td>Friday</td>\n",
       "      <td>success fundrais school chariti initi joy give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>Februari</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>particip multicultur festiv celebr divers musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>November</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>organ virtual talent show challeng time bring ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text           Timestamp  \\\n",
       "0     Enjoying a beautiful day at the park!        ... 2023-01-15 12:30:00   \n",
       "1     Traffic was terrible this morning.           ... 2023-01-15 08:45:00   \n",
       "2     Just finished an amazing workout! ðŸ’ª          ... 2023-01-15 15:45:00   \n",
       "3     Excited about the upcoming weekend getaway!  ... 2023-01-15 18:20:00   \n",
       "4     Trying out a new recipe for dinner tonight.  ... 2023-01-15 19:55:00   \n",
       "..                                                 ...                 ...   \n",
       "727  Collaborating on a science project that receiv... 2017-08-18 18:20:00   \n",
       "728  Attending a surprise birthday party organized ... 2018-06-22 14:15:00   \n",
       "729  Successfully fundraising for a school charity ... 2019-04-05 17:30:00   \n",
       "730  Participating in a multicultural festival, cel... 2020-02-29 20:45:00   \n",
       "731  Organizing a virtual talent show during challe... 2020-11-15 15:15:00   \n",
       "\n",
       "                                      User   Platform  Retweets  Likes  \\\n",
       "0                            User123          Twitter      15.0   30.0   \n",
       "1                            CommuterX        Twitter       5.0   10.0   \n",
       "2                            FitnessFan     Instagram      20.0   40.0   \n",
       "3                            AdventureX      Facebook       8.0   15.0   \n",
       "4                            ChefCook       Instagram      12.0   25.0   \n",
       "..                                     ...        ...       ...    ...   \n",
       "727       ScienceProjectSuccessHighSchool    Facebook      20.0   39.0   \n",
       "728            BirthdayPartyJoyHighSchool   Instagram      25.0   48.0   \n",
       "729   CharityFundraisingTriumphHighSchool     Twitter      22.0   42.0   \n",
       "730    MulticulturalFestivalJoyHighSchool    Facebook      21.0   43.0   \n",
       "731    VirtualTalentShowSuccessHighSchool   Instagram      24.0   47.0   \n",
       "\n",
       "       Country  Year     Month Day_of_Week  \\\n",
       "0          USA  2023   Januari      Sunday   \n",
       "1       Canada  2023   Januari      Sunday   \n",
       "2          USA  2023   Januari      Sunday   \n",
       "3           UK  2023   Januari      Sunday   \n",
       "4    Australia  2023   Januari      Sunday   \n",
       "..         ...   ...       ...         ...   \n",
       "727         UK  2017   Agustus      Friday   \n",
       "728        USA  2018      Juni      Friday   \n",
       "729     Canada  2019     April      Friday   \n",
       "730         UK  2020  Februari    Saturday   \n",
       "731        USA  2020  November      Sunday   \n",
       "\n",
       "                                            Clean_Text  \n",
       "0                                enjoy beauti day park  \n",
       "1                                 traffic terribl morn  \n",
       "2                                  finish amaz workout  \n",
       "3                          excit upcom weekend getaway  \n",
       "4                         tri new recip dinner tonight  \n",
       "..                                                 ...  \n",
       "727  collabor scienc project receiv recognit region...  \n",
       "728  attend surpris birthday parti organ friend sur...  \n",
       "729  success fundrais school chariti initi joy give...  \n",
       "730  particip multicultur festiv celebr divers musi...  \n",
       "731  organ virtual talent show challeng time bring ...  \n",
       "\n",
       "[732 rows x 11 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique values for Platform: 3\n",
      "\u001b[31mInstagram: 258\u001b[39m\n",
      "\u001b[32mTwitter: 243\u001b[39m\n",
      "\u001b[33mFacebook: 231\u001b[39m\n",
      "\n",
      "==============================\n",
      "\n",
      "Total unique values for Country: 33\n",
      "\u001b[31mUSA: 188\u001b[39m\n",
      "\u001b[32mUK: 143\u001b[39m\n",
      "\u001b[33mCanada: 135\u001b[39m\n",
      "\u001b[34mAustralia: 75\u001b[39m\n",
      "\u001b[35mIndia: 70\u001b[39m\n",
      "\u001b[36mBrazil: 17\u001b[39m\n",
      "\u001b[37mFrance: 16\u001b[39m\n",
      "\u001b[90mJapan: 15\u001b[39m\n",
      "\u001b[91mGermany: 14\u001b[39m\n",
      "\u001b[92mItaly: 11\u001b[39m\n",
      "\u001b[31mSpain: 6\u001b[39m\n",
      "\u001b[32mSouth Africa: 6\u001b[39m\n",
      "\u001b[33mGreece: 5\u001b[39m\n",
      "\u001b[34mNetherlands: 4\u001b[39m\n",
      "\u001b[35mSwitzerland: 3\u001b[39m\n",
      "\u001b[36mPortugal: 2\u001b[39m\n",
      "\u001b[37mAustria: 2\u001b[39m\n",
      "\u001b[90mBelgium: 2\u001b[39m\n",
      "\u001b[91mDenmark: 2\u001b[39m\n",
      "\u001b[92mCzech Republic: 2\u001b[39m\n",
      "\u001b[31mSweden: 2\u001b[39m\n",
      "\u001b[32mColombia: 1\u001b[39m\n",
      "\u001b[33mScotland: 1\u001b[39m\n",
      "\u001b[34mKenya: 1\u001b[39m\n",
      "\u001b[35mJamaica: 1\u001b[39m\n",
      "\u001b[36mIreland: 1\u001b[39m\n",
      "\u001b[37mChina: 1\u001b[39m\n",
      "\u001b[90mNorway: 1\u001b[39m\n",
      "\u001b[91mCambodia: 1\u001b[39m\n",
      "\u001b[92mMaldives: 1\u001b[39m\n",
      "\u001b[31mPeru: 1\u001b[39m\n",
      "\u001b[32mJordan: 1\u001b[39m\n",
      "\u001b[33mThailand: 1\u001b[39m\n",
      "\n",
      "==============================\n",
      "\n",
      "Total unique values for Year: 14\n",
      "\u001b[31m2023: 289\u001b[39m\n",
      "\u001b[32m2019: 73\u001b[39m\n",
      "\u001b[33m2020: 69\u001b[39m\n",
      "\u001b[34m2021: 63\u001b[39m\n",
      "\u001b[35m2022: 63\u001b[39m\n",
      "\u001b[36m2018: 56\u001b[39m\n",
      "\u001b[37m2017: 43\u001b[39m\n",
      "\u001b[90m2016: 38\u001b[39m\n",
      "\u001b[91m2015: 19\u001b[39m\n",
      "\u001b[92m2011: 4\u001b[39m\n",
      "\u001b[31m2012: 4\u001b[39m\n",
      "\u001b[32m2013: 4\u001b[39m\n",
      "\u001b[33m2014: 4\u001b[39m\n",
      "\u001b[34m2010: 3\u001b[39m\n",
      "\n",
      "==============================\n",
      "\n",
      "Total unique values for Month: 12\n",
      "\u001b[31mFebruari: 85\u001b[39m\n",
      "\u001b[32mJanuari: 82\u001b[39m\n",
      "\u001b[33mAgustus: 78\u001b[39m\n",
      "\u001b[34mSeptember: 77\u001b[39m\n",
      "\u001b[35mJuni: 71\u001b[39m\n",
      "\u001b[36mJuli: 62\u001b[39m\n",
      "\u001b[37mApril: 51\u001b[39m\n",
      "\u001b[90mNovember: 49\u001b[39m\n",
      "\u001b[91mOktober: 48\u001b[39m\n",
      "\u001b[92mMei: 46\u001b[39m\n",
      "\u001b[31mMaret: 44\u001b[39m\n",
      "\u001b[32mDesember: 39\u001b[39m\n",
      "\n",
      "==============================\n",
      "\n",
      "Total unique values for Day_of_Week: 7\n",
      "\u001b[31mSunday: 119\u001b[39m\n",
      "\u001b[32mSaturday: 115\u001b[39m\n",
      "\u001b[33mTuesday: 110\u001b[39m\n",
      "\u001b[34mFriday: 108\u001b[39m\n",
      "\u001b[35mMonday: 97\u001b[39m\n",
      "\u001b[36mThursday: 95\u001b[39m\n",
      "\u001b[37mWednesday: 88\u001b[39m\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "specified_columns = ['Platform','Country', 'Year','Month','Day_of_Week']\n",
    "\n",
    "for col in specified_columns:\n",
    "    total_unique_values = df[col].nunique()\n",
    "    print(f'Total unique values for {col}: {total_unique_values}')\n",
    "\n",
    "    top_values = df[col].value_counts()\n",
    "\n",
    "    colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE, Fore.LIGHTBLACK_EX, Fore.LIGHTRED_EX, Fore.LIGHTGREEN_EX]\n",
    "\n",
    "    for i, (value, count) in enumerate(top_values.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        print(f'{color}{value}: {count}{Fore.RESET}')\n",
    "\n",
    "    print('\\n' + '=' * 30 + '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy() # Making a copy of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\azeem/nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\azeem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m      3\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVader_Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClean_Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(text)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVader_Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m score: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.05\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\azeem\\anaconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(lexicon_file)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32mc:\\Users\\azeem\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\azeem\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\azeem\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\azeem/nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\azeem\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\azeem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df1['Vader_Score'] = df1['Clean_Text'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "df1['Sentiment'] = df1['Vader_Score'].apply(lambda score: 'positive' if score >= 0.05 else ('negative' if score <= -0.05 else 'neutral'))\n",
    "\n",
    "print(df1[['Clean_Text', 'Vader_Score', 'Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
